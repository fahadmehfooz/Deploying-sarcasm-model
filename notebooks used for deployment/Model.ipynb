{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# I am using logitic regression model while deploying. I am also using the entire dataset to learn when deploying the model to expand the model's learning.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import calendar\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%matplotlib inline\n",
    "import time\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.layers import Conv1D, Flatten,MaxPooling1D\n",
    "from keras.preprocessing import text\n",
    "\n",
    "\n",
    "\n",
    "# Importing the csv data files \n",
    "sarcasm_df = pd.read_csv('train-balanced-sarcasm.csv')\n",
    " \n",
    "# Data Pre-Processing\n",
    "# Removing the null comments\n",
    "sarcasm_df.dropna(subset=['comment'], inplace=True)\n",
    "sarcasm_df['comment'] = sarcasm_df['comment'].str.lower()\n",
    "sarcasm_df['comment'] = sarcasm_df['comment'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "sarcasm_df.reset_index(drop = True, inplace = True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# multinomial logistic regression a.k.a softmax classifier\n",
    "logit = LogisticRegression(random_state= 42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\n",
    "X_tf_idf= vectorizer.fit_transform(sarcasm_df['comment'])\n",
    "\n",
    "logit.fit(X_tf_idf,  sarcasm_df['label'])\n",
    "\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\n",
    "vectorizer=pickle.load(open('vectorizer.pkl','rb'))\n",
    "\n",
    "pickle.dump(logit, open('logit.pkl', 'wb'))\n",
    "logit = pickle.load(open('logit.pkl','rb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
